# Mini-Transformer

A lightweight implementation of the Transformer architecture (1â€“5M parameters) designed for **two tasks** within a single project:
- **Language Modeling (LM)**: next-token prediction with Perplexity evaluation  
- **Text Classification (CLF)**: sequence classification (e.g., sentiment analysis)

 **Goal:** Build a small, interpretable, and efficient Transformer model that can run on standard GPU/CPU hardware, with clean code and reproducible results.

---

## âœ¨ Features
- Shared architecture with two output heads (`task="lm"` or `task="clf"`)  
- Compact design: 2â€“4 Transformer blocks, 128â€“256 hidden dimension, 2â€“4 attention heads  
- Supports BPE/SentencePiece tokenization (8kâ€“16k vocabulary)  
- Training scripts for both LM and classification tasks  
- Evaluation metrics: **Perplexity (LM)**, **Accuracy/F1 (CLF)**  
- Demo inference script for generating text or predicting labels  

---

## ðŸ“‚ Project Structure

mini-transformer/
â”œâ”€ data/ # scripts for dataset download/preprocessing
â”œâ”€ tokenizer/ # train or load BPE tokenizer
â”œâ”€ src/
â”‚ â”œâ”€ model.py # Mini-Transformer architecture
â”‚ â”œâ”€ train_lm.py # training script for language modeling
â”‚ â”œâ”€ train_clf.py # training script for classification
â”‚ â””â”€ infer.py # inference/demo script
â”œâ”€ notebooks/ # demo Jupyter notebooks
â”œâ”€ tests/ # simple unit tests
â””â”€ README.md # project documentation
